
@misc{wolfram_research_inc_wolframscriptwolfram_2024,
	title = {{WolframScript}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/program/wolframscript.html},
	urldate = {2024-09-21},
	author = {{Wolfram Research, Inc.}},
	month = sep,
	year = {2024},
}

@misc{wolfram_research_inc_using_2024,
	title = {Using {Testing} {Notebooks}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/UsingTestingNotebooks.html},
	urldate = {2024-09-21},
	author = {{Wolfram Research, Inc.}},
	month = sep,
	year = {2024},
}

@misc{wolfram_research_inc_testreportwolfram_2024,
	title = {{TestReport}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/TestReport.html},
	urldate = {2024-09-21},
	author = {{Wolfram Research, Inc.}},
	month = sep,
	year = {2024},
}

@misc{noauthor_testobjectwolfram_2024,
	title = {{TestObject}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/TestObject.html},
	urldate = {2024-09-21},
	month = sep,
	year = {2024},
}

@misc{wolfram_research_inc_testevaluatewolfram_2024,
	title = {{TestEvaluate}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/TestEvaluate.html},
	urldate = {2024-09-21},
	author = {{Wolfram Research, Inc.}},
	month = sep,
	year = {2024},
}

@misc{wolfram_research_inc_testcreatewolfram_2024,
	title = {{TestCreate}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/TestCreate.html},
	urldate = {2024-09-21},
	author = {{Wolfram Research, Inc.}},
	month = sep,
	year = {2024},
}

@misc{wolfram_research_inc_using_2024-1,
	title = {Using the {Testing} {Framework}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/UsingTheTestingFramework.html},
	urldate = {2024-09-21},
	author = {{Wolfram Research, Inc.}},
	month = sep,
	year = {2024},
}

@misc{wolfram_research_inc_verificationtestwolfram_2024,
	title = {{VerificationTest}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/VerificationTest.html},
	urldate = {2024-09-21},
	author = {{Wolfram Research, Inc.}},
	month = sep,
	year = {2024},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2024-09-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
}

@misc{wolfram_research_inc_rules_2024,
	title = {Rules \& {Patterns}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/RulesAndPatterns.html},
	urldate = {2023-10-14},
	author = {{Wolfram Research, Inc.}},
	month = aug,
	year = {2024},
}

@misc{michael_george_first_2024,
	title = {First order logic, {Gödel}'s theorem ({CS} 2800, {Spring} 2017)},
	url = {https://www.cs.cornell.edu/courses/cs2800/2017sp/lectures/lec41-godel.html},
	urldate = {2024-08-26},
	author = {{Michael George}},
	month = aug,
	year = {2024},
}

@misc{wolfram_research_inc_mathematica_2024,
	title = {Mathematica: {A} {System} for {Doing} {Mathematics} by {Computer}, {Second} {Edition}},
	url = {https://www.wolfram.com/books/profile.cgi?id=3617},
	urldate = {2024-08-26},
	author = {{Wolfram Research, Inc.}},
	month = aug,
	year = {2024},
}

@misc{wolfram_research_inc_summary_2024,
	title = {Summary of {New} and {Improved} {Features} in 14.0—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/SummaryOfNewFeaturesIn140.html},
	urldate = {2024-02-24},
	author = {{Wolfram Research, Inc.}},
	month = aug,
	year = {2024},
}

@misc{wolfram_research_inc_yet_2024,
	title = {Yet {More} {New} {Ideas} and {New} {Functions}: {Launching} {Version} 14.1 of {Wolfram} {Language} \& {Mathematica}},
	shorttitle = {Yet {More} {New} {Ideas} and {New} {Functions}},
	url = {https://writings.stephenwolfram.com/2024/07/yet-more-new-ideas-and-new-functions-launching-version-14-1-of-wolfram-language-mathematica/},
	abstract = {Version 14.1 gains computational advances for human and AI users. Detailed examples of new and expanded features: semantic search, LLMs, symbolic arrays, binomial coefficients, differential and difference equations, PDEs, biomolecules, neural nets, dates, videos, speech recognition, geography, astronomy, geometry, notebooks, natural language input, diffs, compiler, external languages.},
	language = {en},
	urldate = {2024-08-26},
	author = {{Wolfram Research, Inc.}},
	month = jul,
	year = {2024},
}

@misc{wolfram_research_inc_about_2024,
	title = {About {Wolfram}{\textbar}{Alpha}: {Making} the {World}'s {Knowledge} {Computable}},
	shorttitle = {About {Wolfram}{\textbar}{Alpha}},
	url = {https://www.wolframalpha.com},
	abstract = {Wolfram{\textbar}Alpha brings expert-level knowledge and capabilities to the broadest possible range of people—spanning all professions and education levels.},
	language = {en},
	urldate = {2024-08-26},
	author = {{Wolfram Research, Inc.}},
	month = aug,
	year = {2024},
}

@misc{wolfram_research_inc_expressionswolfram_2024,
	title = {Expressions—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/Expressions.html#20284},
	urldate = {2024-08-26},
	author = {{Wolfram Research, Inc.}},
	month = aug,
	year = {2024},
}

@misc{misc_quaternion_2024,
	title = {Quaternion},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Quaternion&oldid=1235436136},
	abstract = {In mathematics, the quaternion number system extends the complex numbers. Quaternions were first described by the Irish mathematician William Rowan Hamilton in 1843 and applied to mechanics in three-dimensional space. The algebra of quaternions is often denoted by H (for Hamilton), or in blackboard bold by 
  
    
      
        
          H
        
        .
      
    
    \{{\textbackslash}displaystyle {\textbackslash}mathbb \{H\} .\}
  
 Quaternions are not a field, because multiplication of quaternions is not, in general, commutative. Quaternions provide a definition of the quotient of two vectors in a three-dimensional space. Quaternions are generally represented in the form

  
    
      
        a
        +
        b
         
        
          i
        
        +
        c
         
        
          j
        
        +
        d
         
        
          k
        
        ,
      
    
    \{{\textbackslash}displaystyle a+b{\textbackslash} {\textbackslash}mathbf \{i\} +c{\textbackslash} {\textbackslash}mathbf \{j\} +d{\textbackslash} {\textbackslash}mathbf \{k\} ,\}
  

where the coefficients a, b, c, d are real numbers, and 1, i, j, k are the basis vectors or basis elements.
Quaternions are used in pure mathematics, but also have practical uses in applied mathematics, particularly for calculations involving three-dimensional rotations, such as in three-dimensional computer graphics, computer vision, magnetic resonance imaging and crystallographic texture analysis. They can be used alongside other methods of rotation, such as Euler angles and rotation matrices, or as an alternative to them, depending on the application.
In modern terms, quaternions form a four-dimensional associative normed division algebra over the real numbers, and therefore a ring, also a division ring and a domain. It is a special case of a Clifford algebra, classified as 
  
    
      
        
          Cl
          
            0
            ,
            2
          
        
        ⁡
        (
        
          R
        
        )
        ≅
        
          Cl
          
            3
            ,
            0
          
          
            +
          
        
        ⁡
        (
        
          R
        
        )
        .
      
    
    \{{\textbackslash}displaystyle {\textbackslash}operatorname \{Cl\} \_\{0,2\}({\textbackslash}mathbb \{R\} ){\textbackslash}cong {\textbackslash}operatorname \{Cl\} \_\{3,0\}{\textasciicircum}\{+\}({\textbackslash}mathbb \{R\} ).\}
  
 It was the first noncommutative division algebra to be discovered.
According to the Frobenius theorem, the algebra 
  
    
      
        
          H
        
      
    
    \{{\textbackslash}displaystyle {\textbackslash}mathbb \{H\} \}
  
 is one of only two finite-dimensional division rings containing a proper subring isomorphic to the real numbers; the other being the complex numbers. These rings are also Euclidean Hurwitz algebras, of which the quaternions are the largest associative algebra (and hence the largest ring). Further extending the quaternions yields the non-associative octonions, which is the last normed division algebra over the real numbers. The next extension gives the sedenions, which have zero divisors and so cannot be a normed division algebra.
The unit quaternions give a group structure on the 3-sphere S3 isomorphic to the groups Spin(3) and SU(2), i.e. the universal cover group of SO(3). The positive and negative basis vectors form the eight-element quaternion group.},
	language = {en},
	urldate = {2024-08-26},
	journal = {Wikipedia},
	author = {{Misc.}},
	month = jul,
	year = {2024},
	note = {Page Version ID: 1235436136},
}

@misc{wolfram_research_inc_transformation_2024,
	title = {Transformation {Rules} and {Definitions}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/TransformationRulesAndDefinitions.html#6972},
	urldate = {2024-08-26},
	author = {{Wolfram Research, Inc.}},
	month = aug,
	year = {2024},
}

@misc{wolfram_research_inc_contextwolfram_2024,
	title = {\${Context}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/$Context.html},
	urldate = {2024-04-09},
	author = {{Wolfram Research, Inc.}},
	month = aug,
	year = {2024},
}

@misc{wolfram_research_inc_textual_2024,
	title = {Textual {Input} and {Output}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/TextualInputAndOutput.html#12413},
	urldate = {2024-08-24},
	author = {{Wolfram Research, Inc.}},
	month = aug,
	year = {2024},
}

@misc{wolfram_research_inc_messagewolfram_2024,
	title = {Message—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Message.html},
	urldate = {2024-08-24},
	author = {{Wolfram Research, Inc.}},
	month = aug,
	year = {2024},
}

@misc{noauthor_messagewolfram_nodate,
	title = {Message—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Message.html},
	urldate = {2024-02-24},
}

@misc{noauthor_makeboxeswolfram_nodate,
	title = {{MakeBoxes}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/MakeBoxes.html},
	urldate = {2024-07-20},
}

@misc{wolfram_research_inc_makeboxeswolfram_nodate,
	title = {{MakeBoxes}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/MakeBoxes.html},
	urldate = {2024-07-19},
	author = {{Wolfram Research, Inc.}},
}

@misc{noauthor_makeboxeswolfram_nodate-1,
	title = {{MakeBoxes}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/MakeBoxes.html},
	urldate = {2024-07-19},
}

@misc{wolfram_research_inc_wstp_2024,
	title = {{WSTP}: {Wolfram} {Symbolic} {Transfer} {Protocol}},
	shorttitle = {{WSTP}},
	url = {https://www.wolfram.com/wstp/},
	abstract = {Seamlessly communicate code, data, and more between programs. Native protocol for transferring Wolfram Language symbolic expressions between programs.},
	language = {en},
	urldate = {2024-07-10},
	author = {{Wolfram Research, Inc.}},
	month = jul,
	year = {2024},
}

@misc{wolfram_research_inc_wolframkernelwolfram_2024,
	title = {{WolframKernel}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/program/WolframKernel.html},
	urldate = {2024-07-10},
	author = {{Wolfram Research, Inc.}},
	month = jul,
	year = {2024},
}

@misc{noauthor_wolframkernelwolfram_nodate,
	title = {{WolframKernel}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/program/WolframKernel.html},
	urldate = {2024-07-10},
}

@misc{scrum_org_home_2024,
	title = {Home {\textbar} {Scrum}.org},
	url = {https://www.scrum.org/index},
	abstract = {Welcome to the Home of Scrum!™},
	language = {en},
	urldate = {2024-07-02},
	author = {{Scrum Org}},
	year = {2024},
}

@misc{atlassian_bitbucket_2024,
	title = {Bitbucket {\textbar} {Git} solution for teams using {Jira}},
	url = {https://bitbucket.org/product},
	abstract = {Bitbucket Cloud is a Git-based code and CI/CD tool optimized for teams using Jira.},
	language = {en},
	urldate = {2024-07-02},
	journal = {Bitbucket},
	author = {Atlassian},
	year = {2024},
}

@misc{atlassian_jira_2024,
	title = {Jira {\textbar} {Issue} \& {Project} {Tracking} {Software} {\textbar} {Atlassian}},
	url = {https://www.atlassian.com/software/jira},
	urldate = {2024-07-02},
	author = {{Atlassian}},
	year = {2024},
}

@misc{wolfram_research_inc_wolfram_2024,
	title = {Wolfram {Enterprise} {Private} {Cloud} for {Computation}},
	url = {https://www.wolfram.com/enterprise-private-cloud/},
	abstract = {Completely control your data from development to computation to deployment. Entirely housed and managed in your infrastructure. Wolfram technologies.},
	language = {en},
	urldate = {2024-07-02},
	author = {{Wolfram Research, Inc.}},
	month = jul,
	year = {2024},
}

@misc{wolfram_research_inc_wolfram_2024-1,
	title = {Wolfram {Workbench}: {Eclipse}-based {IDE} for {Wolfram} {Language}},
	shorttitle = {Wolfram {Workbench}},
	url = {https://www.wolfram.com/workbench/},
	abstract = {Code editing, navigation, project management tools for enterprise-class development and deployment. Workbench is specialized for Wolfram Language, Mathematica, other Wolfram technologies.},
	language = {en},
	urldate = {2024-07-02},
	author = {{Wolfram Research, Inc.}},
	month = jul,
	year = {2024},
}

@misc{stephen_wolfram_stephen_2024,
	title = {Stephen {Wolfram}: {A} {New} {Kind} of {Science} {\textbar} {Online}—{Table} of {Contents}},
	shorttitle = {Stephen {Wolfram}},
	url = {https://www.wolframscience.com/nks/},
	abstract = {The latest on exploring the computational universe, with free online access to Stephen Wolfram's classic 1,200-page breakthrough book.},
	language = {en},
	urldate = {2024-07-02},
	author = {{Stephen Wolfram}},
	month = jul,
	year = {2024},
}

@misc{stephen_wolfram_stephen_2024-1,
	title = {Stephen {Wolfram}: {Official} {Website}},
	shorttitle = {Stephen {Wolfram}},
	url = {https://www.stephenwolfram.com/},
	abstract = {Official website of Stephen Wolfram: Founder \& CEO of Wolfram Research; creator of Mathematica, Wolfram{\textbar}Alpha \& the Wolfram Language; author of A New Kind of Science},
	language = {en},
	urldate = {2024-07-02},
	author = {{Stephen Wolfram}},
	month = jul,
	year = {2024},
}

@misc{wolfram_research_inc_wolframone_2024,
	title = {Wolfram{\textbar}{One}: {Cloud}-{Desktop} {Computation} {Platform}},
	shorttitle = {Wolfram{\textbar}{One}},
	url = {http://www.wolfram.com/wolfram-one/},
	abstract = {High-level, hybrid computation platform on cloud + desktop. Take your development further with the full capabilities of the Wolfram technology stack.},
	language = {en},
	urldate = {2024-07-02},
	author = {{Wolfram Research, Inc.}},
	month = jul,
	year = {2024},
}

@misc{wolfram_research_inc_wolfram_2024-2,
	title = {Wolfram {Language}: {Programming} {Language} + {Built}-{In} {Knowledge}},
	shorttitle = {Wolfram {Language}},
	url = {https://www.wolfram.com/language/},
	abstract = {Symbolic programming language to express complex ideas in computational form. Efficiently develop powerful programs through Wolfram Notebooks or command-line access.},
	language = {en},
	urldate = {2024-07-02},
	author = {{Wolfram Research, Inc.}},
	month = jul,
	year = {2024},
}

@misc{wolfram_research_inc_wolfram_2024-3,
	title = {Wolfram {Summer} {School}},
	url = {https://education.wolfram.com/summer-school},
	abstract = {Learn to apply Wolfram's unique approach to turning ideas into projects, products and companies. Tracks for physics, education, science \& technology.},
	language = {en},
	urldate = {2024-06-10},
	author = {{Wolfram Research, Inc.}},
	month = jul,
	year = {2024},
}

@misc{wolfram_research_inc_wolfram_2024-4,
	title = {Wolfram: {Computation} {Meets} {Knowledge}},
	shorttitle = {Wolfram {Homepage}},
	url = {https://www.wolfram.com/},
	abstract = {Wolfram, creators of the Wolfram Language, Wolfram{\textbar}Alpha, Mathematica, Development Platform, Data Science Platform, Finance Platform, SystemModeler...},
	language = {en},
	urldate = {2024-07-02},
	author = {{Wolfram Research, Inc.}},
	month = jul,
	year = {2024},
	keywords = {wolframcomp},
}

@misc{wolfram_research_inc_mathematica_2024-1,
	title = {Mathematica {Homepage}},
	shorttitle = {Wolfram},
	url = {https://www.wolfram.com/mathematica/},
	abstract = {Mathematica: high-powered computation with thousands of Wolfram Language functions, natural language input, real-world data, mobile support.},
	language = {en},
	urldate = {2024-07-02},
	author = {{Wolfram Research, Inc.}},
	month = jul,
	year = {2024},
}

@misc{noauthor_wolfram_nodate,
	title = {Wolfram {Research}, {Inc}.},
	url = {https://reference.wolfram.com/legacy/v3/MainBook/2.5.1.html},
	urldate = {2024-04-03},
}

@misc{american_guy_in_germany_how_2023,
	title = {{HOW} {TO} {FILE} your {US} {Expat} {Taxes} for {FREE}: 2555 {Foreign} {Earned} {Income} {Exclusion} and {More}},
	shorttitle = {{HOW} {TO} {FILE} your {US} {Expat} {Taxes} for {FREE}},
	url = {https://www.youtube.com/watch?v=HA2wDMIZakc},
	abstract = {As a USA citizen, you still have to file taxes with the IRS every year, even if you don't owe any taxes. In this video, I show you an example of how I file taxes, and use the 2555 form to exclude all my foreign earned income! The IRS provides telephone advice, so don't hesitate to call them for help if you need it. Here are the required forms.

https://www.irs.gov/pub/irs-pdf/f1040...
https://www.irs.gov/pub/irs-pdf/f2555...
https://www.irs.gov/pub/irs-pdf/f1040...
https://www.irs.gov/individuals/inter...
https://www.irs.gov/individuals/inter...

Check out my other content on Linktree! https://linktr.ee/americanguyingermany

------------------------------Products used/recommended in this video--------------------------------------------------
         (these are affiliate links that help fund videos like this, at no extra cost to you)

How I learned German: https://amzn.to/3IMv4yA

My Gear: https://kit.co/tomscoffeecorner/cheap...

I'm an American Expat, originally from Wisconsin (settled by primarily Germans) now living in Rosenheim, Germany, documenting what I think is interesting here. I arrived long ago as a master's student, and love has kept me here.

Chapters:
0:00 Intro
0:14 Do you really have to file taxes from abroad?
1:09 IRS 1040 Form
2:45 IRS 2555 Form: Foreign Earned Income Exclusion
6:25 Foreign Earned Income Official Exchange Rates
8:54 IRS Schedule 1 Form
9:52 Finishing the 1040 Form
12:45 Where to send the Forms: IRS Address

----------------------------
DISCLOSURE: Some of the links on this page are affiliate links, meaning, at no additional cost to you, I may earn a commission if you click through and make a purchase and/or subscribe. Affiliate commissions help fund videos like this one.
----------------------------
Affiliatelinks/Werbelinks:
Einige der oben genannte Links sind sogenannte Affiliate-Links. Wenn du auf so einen Affiliate-Link klickst und über diesen Link einkaufst, bekomme ich von dem betreffenden Online-Shop oder Anbieter eine Provision. Für dich verändert sich der Preis nicht.

\#AmericanGuyInGermany \#LifeInGermany \#ExpatLife},
	urldate = {2024-04-16},
	author = {{American Guy in Germany}},
	month = mar,
	year = {2023},
}

@misc{noauthor_institute_nodate,
	title = {The {Institute}},
	url = {http://www.oeaw.ac.at/ricam/institute/about},
	language = {en-US},
	urldate = {2024-04-10},
}

@article{marin_rule-based_nodate,
	title = {Rule-{Based} {Programming} with {Mathematica}},
	abstract = {Recent years have witnessed renewed developments of the rule-based programming style, addressing both its theoretical foundations and its practical implementations. New rule-based programming languages have emerged, and several practical applications have shown that rules are indeed a useful programming tool. We believe that Mathematica has the right basic ingredients for supporting rule-based programming eﬃciently. Because the main features of a true rule-based programming language are still missing, we developed a Mathematica package, ρLog, which enables advanced rule-based programming within Mathematica. We describe here the capabilities of ρLog and illustrate its usage with several examples.},
	language = {en},
	author = {Marin, Mircea and Piroi, Florina},
}

@article{marin_rule-based_nodate-1,
	title = {Rule-{Based} {Programming} with {Mathematica}},
	abstract = {Recent years have witnessed renewed developments of the rule-based programming style, addressing both its theoretical foundations and its practical implementations. New rule-based programming languages have emerged, and several practical applications have shown that rules are indeed a useful programming tool. We believe that Mathematica has the right basic ingredients for supporting rule-based programming eﬃciently. Because the main features of a true rule-based programming language are still missing, we developed a Mathematica package, ρLog, which enables advanced rule-based programming within Mathematica. We describe here the capabilities of ρLog and illustrate its usage with several examples.},
	language = {en},
	author = {Marin, Mircea and Piroi, Florina},
}

@misc{noauthor_real-time_nodate,
	title = {Real-{Time} {3D} {Development} {Platform} \& {Editor}},
	url = {https://unity.com/products/unity-engine},
	abstract = {Unity’s real-time 3D development platform lets artists, designers, and developers collaborate to create immersive \& interactive games! Try Unity today!},
	language = {en},
	urldate = {2024-04-10},
	journal = {Unity},
}

@misc{noauthor_highlevel_nodate,
	title = {High–{Level} {Functions}: {New} in {Wolfram} {Language} 12},
	shorttitle = {High–{Level} {Functions}},
	url = {https://www.wolfram.com/language/12/built-in-interface-to-unity-game-engine/high-level-functions.html?product=language},
	language = {en},
	urldate = {2024-04-10},
}

@misc{noauthor_gnu_nodate,
	title = {The {GNU} {General} {Public} {License} v3.0 - {GNU} {Project} - {Free} {Software} {Foundation}},
	url = {https://www.gnu.org/licenses/gpl-3.0.en.html},
	urldate = {2024-04-10},
}

@misc{noauthor_gnu_nodate-1,
	title = {The {GNU} {Operating} {System} and the {Free} {Software} {Movement}},
	url = {https://www.gnu.org/home.en.html},
	urldate = {2024-04-10},
}

@misc{noauthor_needswolfram_nodate,
	title = {Needs—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Needs.html},
	urldate = {2024-04-09},
}

@misc{noauthor_procedural_nodate,
	title = {Procedural {Programming}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/ProceduralProgramming.html},
	urldate = {2024-04-09},
}

@misc{noauthor_language_nodate,
	title = {Language {Overview}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/LanguageOverview.html},
	urldate = {2024-04-09},
}

@misc{noauthor_getwolfram_nodate,
	title = {Get—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Get.html},
	urldate = {2024-04-09},
}

@misc{noauthor_endwolfram_nodate,
	title = {End—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/End.html},
	urldate = {2024-04-09},
}

@misc{noauthor_beginwolfram_nodate,
	title = {Begin—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Begin.html},
	urldate = {2024-04-09},
}

@misc{noauthor_endpackagewolfram_nodate,
	title = {{EndPackage}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/EndPackage.html},
	urldate = {2024-04-09},
}

@misc{noauthor_beginpackagewolfram_nodate,
	title = {{BeginPackage}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/BeginPackage.html},
	urldate = {2024-04-09},
}

@misc{noauthor_namespace_nodate,
	title = {Namespace {Management}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/NamespaceManagement.html},
	urldate = {2024-04-09},
}

@misc{noauthor_scoping_nodate,
	title = {Scoping {Constructs}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/ScopingConstructs.html},
	urldate = {2024-04-09},
}

@misc{noauthor_package_nodate,
	title = {Package {Development}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/PackageDevelopment.html},
	urldate = {2024-04-09},
}

@misc{noauthor_namespace_nodate-1,
	title = {Namespace {Management}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/NamespaceManagement.html},
	urldate = {2024-04-09},
}

@misc{noauthor_scoping_nodate-1,
	title = {Scoping {Constructs}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/ScopingConstructs.html},
	urldate = {2024-04-09},
}

@misc{noauthor_package_nodate-1,
	title = {Package {Development}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/PackageDevelopment.html},
	urldate = {2024-04-09},
}

@misc{noauthor_building_nodate,
	title = {Building {Large} {Software} {Systems} in the {Wolfram} {Language}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/BuildingLargeSoftwareSystemsInTheWolframLanguage.html},
	urldate = {2024-04-09},
}

@misc{noauthor_internals_nodate,
	title = {The {Internals} of the {Wolfram} {System}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/TheInternalsOfTheWolframSystem.html#28134},
	urldate = {2024-04-09},
}

@misc{noauthor_theorematheoremapackagetemplatem_nodate,
	title = {Theorema/{Theorema}/{PackageTemplate}.m at master · windsteiger/{Theorema}},
	url = {https://github.com/windsteiger/Theorema/blob/master/Theorema/PackageTemplate.m},
	urldate = {2024-04-05},
}

@misc{noauthor_story_2024,
	title = {The {Story} {Continues}: {Announcing} {Version} 14 of {Wolfram} {Language} and {Mathematica}},
	shorttitle = {The {Story} {Continues}},
	url = {https://writings.stephenwolfram.com/2024/01/the-story-continues-announcing-version-14-of-wolfram-language-and-mathematica/},
	abstract = {Wolfram's Version 14 includes expansive connections to and from LLMs. New learning tools, including courses and books. Plus, functionality advances in math, core language, video objects, trees, finite fields, Knowledgebase, systems engineering, graphics. Astro, biological and chemical computation.},
	language = {en},
	urldate = {2024-04-05},
	month = jan,
	year = {2024},
}

@misc{noauthor_stylesheetswolfram_nodate,
	title = {Stylesheets—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/Stylesheets.html},
	urldate = {2024-04-05},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction to {Dynamic}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/IntroductionToDynamic.html},
	urldate = {2024-04-05},
}

@misc{noauthor_findequationalproofwolfram_nodate,
	title = {{FindEquationalProof}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/FindEquationalProof.html},
	urldate = {2024-04-05},
}

@misc{noauthor_proofobjectwolfram_nodate,
	title = {{ProofObject}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/ProofObject.html},
	urldate = {2024-04-05},
}

@misc{noauthor_solve_nodate,
	title = {Solve: {Find} a solution to an equation—{Wolfram} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Solve.html},
	urldate = {2024-04-05},
}

@incollection{g_mayrhofer_s_saminger__w_winsteiger_theorema_nodate,
	title = {Theorema},
	url = {https://www.cs.ru.nl/~freek/comparison/comparison.pdf},
	urldate = {2024-04-05},
	booktitle = {The {Seventeen} {Provers} of the {World}},
	publisher = {Springer},
	author = {{G. Mayrhofer, S. Saminger \% W. Winsteiger}},
}

@misc{noauthor_httpswww3riscjkuatresearchtheoremasoftware_nodate,
	title = {https://www3.risc.jku.at/research/theorema/software/},
	url = {https://www3.risc.jku.at/research/theorema/software/},
	urldate = {2024-04-05},
}

@inproceedings{windsteiger_theorema_2017,
	address = {Timisoara},
	title = {Theorema 2.0: {A} {Brief} {Tutorial}},
	isbn = {978-1-5386-2626-9},
	shorttitle = {Theorema 2.0},
	url = {https://ieeexplore.ieee.org/document/8531262/},
	doi = {10.1109/SYNASC.2017.00016},
	language = {en},
	urldate = {2024-04-05},
	booktitle = {2017 19th {International} {Symposium} on {Symbolic} and {Numeric} {Algorithms} for {Scientific} {Computing} ({SYNASC})},
	publisher = {IEEE},
	author = {Windsteiger, Wolfgang},
	month = sep,
	year = {2017},
	pages = {36--38},
}

@misc{noauthor_wolfram_nodate-1,
	title = {Wolfram {Research}, {Inc}.},
	url = {https://reference.wolfram.com/legacy/v3/MainBook/2.4.10.html},
	urldate = {2024-04-04},
}

@misc{noauthor_inversefunctionwolfram_nodate,
	title = {{InverseFunction}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/InverseFunction.html},
	urldate = {2024-04-04},
}

@misc{noauthor_inversefunctionwolfram_nodate-1,
	title = {{InverseFunction}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/InverseFunction.html},
	urldate = {2024-04-04},
}

@misc{noauthor_map_nodate,
	title = {Map: {Apply} a function to elements in a list—{Wolfram} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Map.html},
	urldate = {2024-04-04},
}

@misc{noauthor_functional_nodate,
	title = {Functional {Operations}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/FunctionalOperations.html},
	urldate = {2024-04-04},
}

@misc{noauthor_pure_nodate,
	title = {Pure {Anonymous} {Function}: {Elementary} {Introduction} to the {Wolfram} {Language}},
	shorttitle = {Pure {Anonymous} {Function}},
	url = {https://www.wolfram.com/language/elementary-introduction/2nd-ed/26-pure-anonymous-functions.html},
	abstract = {Learn to use pure functions in Wolfram Language code, thus simplifying your code and avoiding repetition. Written by Stephen Wolfram.},
	language = {en},
	urldate = {2024-04-04},
}

@misc{noauthor_optionswolfram_nodate,
	title = {Options—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Options.html},
	urldate = {2024-04-03},
}

@misc{noauthor_wolfram_nodate-2,
	title = {Wolfram {Research}, {Inc}.},
	url = {https://reference.wolfram.com/legacy/v3/MainBook/2.5.11.html},
	urldate = {2024-04-03},
}

@misc{noauthor_evaluationwolfram_nodate,
	title = {Evaluation—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/Evaluation.html},
	urldate = {2024-04-03},
}

@misc{noauthor_mathematica_nodate,
	title = {The {Mathematica} {Book}, by {Stephen} {Wolfram}},
	url = {https://www.stephenwolfram.com/publications/mathematica-book/},
	abstract = {Bibliographic publication history of The Mathematica Book, the groundbreaking documentation for Mathematica software, with links to online versions},
	language = {en},
	urldate = {2024-04-03},
}

@misc{noauthor_suppress_nodate,
	title = {Suppress the {Output} of a {Computation}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/howto/SuppressTheOutputOfAComputation.html},
	urldate = {2024-04-03},
}

@misc{noauthor_software_nodate,
	title = {Software},
	url = {https://risc.jku.at/software/},
	abstract = {Software Symbolic computation can be seen as the automation and algorithmization of mathematics. Therefore, most of what we do results in concrete software.},
	language = {en-US},
	urldate = {2024-04-03},
	journal = {RISC - Johannes Kepler University},
}

@misc{noauthor_wolfram_nodate-3,
	title = {Wolfram {Language} {Q}\&{A}},
	url = {https://www.wolfram.com/language/faq/#:~:text=How%20is%20it%20different%20from,deployment%20and%20many%20new%20ideas.},
	abstract = {Platforms, ease of use, graphics, user community, programming buzzwords, licensing, interactivity, communication with other languages, speed, scalability...},
	language = {en},
	urldate = {2024-04-03},
}

@misc{noauthor_why_2019,
	title = {Why {Wolfram} {Tech} {Isn}’t {Open} {Source}—{A} {Dozen} {Reasons}—{Wolfram} {Blog}},
	url = {https://blog.wolfram.com/2019/04/02/why-wolfram-tech-isnt-open-source-a-dozen-reasons/},
	abstract = {Wolfram's Director of Technical Communication \& Strategy, Jon McLoone, shares his thoughts on why it would not have been possible to create the Wolfram technology stack using a free and open-source model.},
	language = {en},
	urldate = {2024-04-02},
	month = apr,
	year = {2019},
}

@misc{noauthor_why_2022,
	type = {Reddit {Post}},
	title = {Why isn't {Wolfram} more popular?},
	url = {www.reddit.com/r/math/comments/ye2vj0/why_isnt_wolfram_more_popular/},
	urldate = {2024-04-02},
	journal = {r/math},
	month = oct,
	year = {2022},
}

@misc{noauthor_wolfram_nodate-4,
	title = {Wolfram {Open} {Code}, {Open} {Source}, {Open} {Data}, {Open} {Resources}},
	url = {http://www.wolfram.com/open-materials/},
	abstract = {Open code, open source, open data--resources from Wolfram. Access Wolfram{\textbar}Alpha, Wolfram Language, documentation, livestreams, Demonstrations, MathWorld. Repositories of data sets, functions, neural net models, notebooks},
	language = {en},
	urldate = {2024-04-02},
}

@misc{noauthor_wolfram_nodate-5,
	title = {Wolfram {Language}: {Programming} {Language} + {Built}-{In} {Knowledge}},
	shorttitle = {Wolfram {Language}},
	url = {https://www.wolfram.com/language/},
	abstract = {Symbolic programming language to express complex ideas in computational form. Efficiently develop powerful programs through Wolfram Notebooks or command-line access.},
	language = {en},
	urldate = {2024-04-02},
}

@misc{noauthor_wolfram_nodate-6,
	title = {Wolfram {Technologies}: {Building} the {Future} through {Technology}},
	shorttitle = {Wolfram {Technologies}},
	url = {https://www.wolfram.com/technologies/},
	abstract = {Technologies from Wolfram for programming, computable documents, data framework, cloud, knowledge, universal deployment, natural language understanding},
	language = {en},
	urldate = {2024-04-02},
}

@misc{noauthor_wolframalpha_nodate,
	title = {Wolfram{\textbar}{Alpha} {Tour}},
	url = {https://www.wolframalpha.com},
	abstract = {Wolfram{\textbar}Alpha brings expert-level knowledge and capabilities to the broadest possible range of people—spanning all professions and education levels.},
	language = {en},
	urldate = {2024-04-02},
}

@misc{noauthor_wolfram_nodate-7,
	title = {Wolfram {Cloud} {App}: {Optimized} {Mobile} {Experience} for {Wolfram} {Cloud}},
	shorttitle = {Wolfram {Cloud} {App}},
	url = {http://www.wolfram.com/cloud-app/},
	abstract = {View your Wolfram Language deployments on the go with the Wolfram Cloud mobile app. Write, test, deploy directly in the cloud and access via mobile.},
	language = {en},
	urldate = {2024-04-02},
}

@misc{noauthor_wolframone_nodate,
	title = {Wolfram{\textbar}{One}: {Cloud}-{Desktop} {Computation} {Platform}},
	shorttitle = {Wolfram{\textbar}{One}},
	url = {http://www.wolfram.com/wolfram-one/},
	abstract = {High-level, hybrid computation platform on cloud + desktop. Take your development further with the full capabilities of the Wolfram technology stack.},
	language = {en},
	urldate = {2024-04-02},
}

@misc{noauthor_wolfram_nodate-8,
	title = {Wolfram {Mathematica}: {Modern} {Technical} {Computing}},
	shorttitle = {Wolfram {Mathematica}},
	url = {https://www.wolfram.com/mathematica/},
	abstract = {Mathematica: high-powered computation with thousands of Wolfram Language functions, natural language input, real-world data, mobile support.},
	language = {en},
	urldate = {2024-04-02},
}

@misc{noauthor_expression_nodate,
	title = {Expression {Structure}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/ExpressionStructure.html},
	urldate = {2024-04-02},
}

@misc{noauthor_headwolfram_nodate,
	title = {Head—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Head.html},
	urldate = {2024-04-02},
}

@misc{noauthor_citywolfram_nodate,
	title = {City—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/entity/City.html},
	urldate = {2024-04-02},
}

@misc{noauthor_planetwolfram_nodate,
	title = {Planet—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/entity/Planet.html},
	urldate = {2024-04-02},
}

@misc{noauthor_curated_nodate,
	title = {Curated {Computational} {Data} in the {Wolfram} {Knowledgebase}},
	url = {https://www.wolframalpha.com},
	abstract = {Comprehensive list of Wolfram{\textbar}Alpha’s computable data topics. Built from the extensive Wolfram Knowledgebase repository. Wide breadth of coverage from science \& technology to society \& culture.},
	language = {en},
	urldate = {2024-04-02},
}

@misc{noauthor_wolfram_nodate,
	title = {Wolfram {Knowledgebase}: {Making} the {Knowledge} of the {World} {Computable}},
	shorttitle = {Wolfram {Knowledgebase}},
	url = {https://www.wolfram.com/knowledgebase/},
	abstract = {Largest, broadest repository of computable knowledge powers Wolfram{\textbar}Alpha, Wolfram Language. Carefully curated knowledge directly derived from primary sources.},
	language = {en},
	urldate = {2024-04-02},
}

@misc{noauthor_boxdatawolfram_nodate,
	title = {{BoxData}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/BoxData.html#:~:text=BoxData%20is%20typically%20used%20as,need%20to%20be%20used%20directly.},
	urldate = {2024-04-02},
}

@misc{noauthor_find_nodate,
	title = {Find the {Underlying} {Box} {Structure} of a {Formatted} {Expression}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/workflow/FindTheUnderlyingBoxStructureOfAFormattedExpression.html},
	urldate = {2024-04-02},
}

@misc{noauthor_cell_nodate,
	title = {Cell {\textgreater} {Show} {Expression}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/menuitem/ShowExpression.html},
	urldate = {2024-04-02},
}

@misc{noauthor_wolfram_nodate-1,
	title = {Wolfram {Notebooks}: {Environment} for {Technical} {Workflows}},
	shorttitle = {Wolfram {Notebooks}},
	url = {http://www.wolfram.com/notebooks/},
	abstract = {Powerful interactive document that supports live computation, dynamic interfaces, full typeset input, image input, automatic code annotation, high-level programmatic interface, thousands of functions and options.},
	language = {en},
	urldate = {2024-04-02},
}

@misc{noauthor_cellwolfram_nodate,
	title = {Cell—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Cell.html},
	urldate = {2024-04-02},
}

@misc{noauthor_notebookwolfram_nodate,
	title = {Notebook—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Notebook.html},
	urldate = {2024-04-02},
}

@misc{noauthor_toexpressionwolfram_nodate,
	title = {{ToExpression}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/ToExpression.html#:~:text=gives%20the%20expression%20obtained%20by,boxes%20as%20Wolfram%20Language%20input.&text=uses%20interpretation%20rules%20corresponding%20to%20the%20specified%20form.&text=wraps%20the%20head%20h%20around%20the%20expression%20produced%20before%20evaluating%20it.},
	urldate = {2024-04-02},
}

@misc{noauthor_expressionswolfram_nodate,
	title = {Expressions—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/Expressions.html#:~:text=Expressions%20in%20the%20Wolfram%20System,6%2D1%5D%20performs%20factorization.},
	urldate = {2024-04-02},
}

@misc{noauthor_expressionswolfram_nodate-1,
	title = {Expressions—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/Expressions.html},
	urldate = {2024-04-02},
}

@misc{noauthor_american_nodate,
	title = {American {Mathematical} {Society}},
	url = {https://www.ams.org/arc/resources/amslatex-about.html},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	urldate = {2024-04-02},
	journal = {American Mathematical Society},
}

@misc{noauthor_latex_nodate,
	title = {{LaTeX} {Project} {Team}},
	url = {https://www.latex-project.org/about/team/},
	urldate = {2024-04-02},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction to {LaTeX}},
	url = {https://www.latex-project.org/about/},
	urldate = {2024-04-02},
}

@misc{noauthor_latex_nodate-1,
	title = {{LaTeX} {Project} {Team}},
	url = {https://www.latex-project.org/about/team/#frank-mittelbach},
	urldate = {2024-04-02},
}

@misc{noauthor_latex_nodate-2,
	title = {{LaTeX} - {A} document preparation system},
	url = {https://www.latex-project.org/},
	urldate = {2024-04-02},
}

@misc{noauthor_wolfram_nodate-2,
	title = {Wolfram {Workbench}: {Eclipse}-based {IDE} for the {Wolfram} {Language}},
	url = {https://www.wolfram.com/workbench/},
	urldate = {2024-04-02},
}

@misc{noauthor_programmers_nodate,
	title = {Programmers {Guidelines}},
	url = {https://www3.risc.jku.at/research/theorema/software/documentation/tutorial/ProgrammersGuidelines.html},
	urldate = {2024-04-02},
}

@misc{noauthor_github_nodate,
	title = {{GitHub} - windsteiger/{Theorema}: {Theorema}: {A} {System} for {Automated} {Reasoning} ({Theorem} {Proving}) and {Automated} {Theory} {Exploration} based on {Mathematica}},
	url = {https://github.com/windsteiger/Theorema},
	urldate = {2024-04-02},
}

@misc{noauthor_download_nodate,
	title = {Download {Theorema} {Package}},
	url = {http://www3.risc.jku.at/research/theorema/software/download-package.php},
	urldate = {2024-04-02},
}

@misc{noauthor_notitle_nodate,
}

@misc{daniluk_frustratingly_2017,
	title = {Frustratingly {Short} {Attention} {Spans} in {Neural} {Language} {Modeling}},
	url = {http://arxiv.org/abs/1702.04521},
	doi = {10.48550/arXiv.1702.04521},
	abstract = {Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memory-augmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.},
	urldate = {2024-03-04},
	publisher = {arXiv},
	author = {Daniluk, Michał and Rocktäschel, Tim and Welbl, Johannes and Riedel, Sebastian},
	month = feb,
	year = {2017},
	note = {arXiv:1702.04521 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{cheng_long_2016,
	title = {Long {Short}-{Term} {Memory}-{Networks} for {Machine} {Reading}},
	url = {https://arxiv.org/abs/1601.06733v7},
	abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
	language = {en},
	urldate = {2024-03-04},
	journal = {arXiv.org},
	author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
	month = jan,
	year = {2016},
}

@misc{luong_effective_2015,
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1508.04025v5},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	language = {en},
	urldate = {2024-03-04},
	journal = {arXiv.org},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	month = aug,
	year = {2015},
}

@misc{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {https://arxiv.org/abs/1409.0473v7},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	language = {en},
	urldate = {2024-03-04},
	journal = {arXiv.org},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
}

@article{suwajanakorn_synthesizing_2017,
	title = {Synthesizing {Obama}: learning lip sync from audio},
	volume = {36},
	issn = {0730-0301, 1557-7368},
	shorttitle = {Synthesizing {Obama}},
	url = {https://dl.acm.org/doi/10.1145/3072959.3073640},
	doi = {10.1145/3072959.3073640},
	abstract = {Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.},
	language = {en},
	number = {4},
	urldate = {2024-03-03},
	journal = {ACM Transactions on Graphics},
	author = {Suwajanakorn, Supasorn and Seitz, Steven M. and Kemelmacher-Shlizerman, Ira},
	month = aug,
	year = {2017},
	pages = {1--13},
}

@misc{kiros_unifying_2014,
	title = {Unifying {Visual}-{Semantic} {Embeddings} with {Multimodal} {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/1411.2539},
	doi = {10.48550/arXiv.1411.2539},
	abstract = {Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars. Sample captions generated for 800 images are made available for comparison.},
	urldate = {2024-03-03},
	publisher = {arXiv},
	author = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Richard S.},
	month = nov,
	year = {2014},
	note = {arXiv:1411.2539 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{karpathy_deep_nodate,
	title = {Deep {Visual}-{Semantic} {Alignments} for {Generating} {Image} {Descriptions}},
	abstract = {We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions signiﬁcantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
	language = {en},
	author = {Karpathy, Andrej and Fei-Fei, Li},
}

@misc{bengio_representation_2014,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	shorttitle = {Representation {Learning}},
	url = {http://arxiv.org/abs/1206.5538},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	urldate = {2024-03-03},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = apr,
	year = {2014},
	note = {arXiv:1206.5538 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{radford_learning_2017,
	title = {Learning to {Generate} {Reviews} and {Discovering} {Sentiment}},
	url = {http://arxiv.org/abs/1704.01444},
	doi = {10.48550/arXiv.1704.01444},
	abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
	urldate = {2024-03-03},
	publisher = {arXiv},
	author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
	month = apr,
	year = {2017},
	note = {arXiv:1704.01444 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{graves_generating_2014,
	title = {Generating {Sequences} {With} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1308.0850},
	doi = {10.48550/arXiv.1308.0850},
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	urldate = {2024-03-03},
	publisher = {arXiv},
	author = {Graves, Alex},
	month = jun,
	year = {2014},
	note = {arXiv:1308.0850 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{papineni_bleu_2002,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2024-03-02},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	editor = {Isabelle, Pierre and Charniak, Eugene and Lin, Dekang},
	month = jul,
	year = {2002},
	pages = {311--318},
}

@misc{noauthor_files_nodate,
	title = {Files, {Streams}, and {External} {Operations}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/FilesStreamsAndExternalOperations.html#14387},
	urldate = {2024-02-28},
}

@misc{noauthor_failurewolfram_nodate,
	title = {Failure—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Failure.html},
	urldate = {2024-02-28},
}

@misc{noauthor_failedwolfram_nodate,
	title = {\${Failed}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/$Failed.html},
	urldate = {2024-02-28},
}

@misc{noauthor_messagewolfram_nodate,
	title = {Message—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Message.html},
	urldate = {2024-02-28},
}

@misc{pathak_curiosity-driven_2017,
	title = {Curiosity-driven {Exploration} by {Self}-supervised {Prediction}},
	url = {http://arxiv.org/abs/1705.05363},
	doi = {10.48550/arXiv.1705.05363},
	abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
	urldate = {2024-02-27},
	publisher = {arXiv},
	author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
	month = may,
	year = {2017},
	note = {arXiv:1705.05363 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{noauthor_operator_nodate,
	title = {Operator {Input} {Forms}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/OperatorInputForms.html},
	urldate = {2024-02-24},
}

@misc{noauthor_what_2019,
	title = {What {We}’ve {Built} {Is} a {Computational} {Language} (and {That}’s {Very} {Important}!)—{Stephen} {Wolfram} {Writings}},
	url = {https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/},
	abstract = {Is the Wolfram Language a computer programming language? Stephen Wolfram explains why it is, rather, a computational language--a way to communicate computational ideas.},
	language = {en},
	urldate = {2024-02-24},
	month = may,
	year = {2019},
}

@misc{noauthor_functional_nodate,
	title = {Functional {Programming}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/FunctionalProgramming.html},
	urldate = {2024-02-24},
}

@misc{noauthor_if_nodate,
	title = {If: {Test} a condition—{Wolfram} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/If.html},
	urldate = {2024-02-24},
}

@misc{noauthor_checkwolfram_nodate,
	title = {Check—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Check.html},
	urldate = {2024-02-24},
}

@misc{noauthor_matchqwolfram_nodate,
	title = {{MatchQ}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/MatchQ.html},
	urldate = {2024-02-24},
}

@misc{noauthor_what_2019-1,
	title = {What {We}’ve {Built} {Is} a {Computational} {Language} (and {That}’s {Very} {Important}!)—{Stephen} {Wolfram} {Writings}},
	url = {https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/},
	abstract = {Is the Wolfram Language a computer programming language? Stephen Wolfram explains why it is, rather, a computational language--a way to communicate computational ideas.},
	language = {en},
	urldate = {2024-02-24},
	month = may,
	year = {2019},
}

@misc{noauthor_blockwolfram_nodate,
	title = {Block—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolframcloud.com/language/ref/Block.html},
	urldate = {2024-02-24},
}

@misc{noauthor_module_nodate,
	title = {Module: {Create} a scoping construct for local variables—{Wolfram} {Documentation}},
	url = {https://reference.wolframcloud.com/language/ref/Module.html},
	urldate = {2024-02-24},
}

@misc{noauthor_compoundexpressionwolfram_nodate,
	title = {{CompoundExpression}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolframcloud.com/language/ref/CompoundExpression.html},
	urldate = {2024-02-24},
}

@misc{noauthor_blanknullsequencewolfram_nodate,
	title = {{BlankNullSequence}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/BlankNullSequence.html},
	urldate = {2024-02-24},
}

@misc{noauthor_blanksequencewolfram_nodate,
	title = {{BlankSequence}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/BlankSequence.html},
	urldate = {2024-02-24},
}

@misc{noauthor_blankwolfram_nodate,
	title = {Blank—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Blank.html},
	urldate = {2024-02-24},
}

@misc{noauthor_package_nodate,
	title = {Package {Development}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/PackageDevelopment.html},
	urldate = {2024-02-24},
}

@misc{noauthor_quietwolfram_nodate,
	title = {Quiet—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Quiet.html},
	urldate = {2024-02-24},
}

@misc{noauthor_connect_nodate,
	title = {Connect to {Other} {Systems}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/howto/ConnectToOtherSystems.html},
	urldate = {2024-02-24},
}

@misc{noauthor_connect_nodate-1,
	title = {Connect a {Java} {Program} to the {Wolfram} {Language}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/howto/ConnectAJavaProgramToTheWolframLanguage.html},
	urldate = {2024-02-24},
}

@misc{noauthor_wolfram_nodate-3,
	title = {Wolfram {Mathematica}: {Modern} {Technical} {Computing}},
	shorttitle = {Wolfram {Mathematica}},
	url = {https://www.wolfram.com/mathematica/},
	abstract = {Mathematica: high-powered computation with thousands of Wolfram Language functions, natural language input, real-world data, mobile support.},
	language = {en},
	urldate = {2024-02-24},
}

@misc{noauthor_intellij_nodate,
	title = {{IntelliJ} {IDEA} – the {Leading} {Java} and {Kotlin} {IDE}},
	url = {https://www.jetbrains.com/idea/promo/},
	abstract = {IntelliJ IDEA is undoubtedly the top-choice IDE for software developers. It makes Java and Kotlin development a more productive and enjoyable experience.},
	language = {en},
	urldate = {2024-02-24},
	journal = {JetBrains},
}

@misc{noauthor_wolfram_nodate-4,
	title = {Wolfram {Language} - {IntelliJ} {IDEs} {Plugin} {\textbar} {Marketplace}},
	url = {https://plugins.jetbrains.com/plugin/7232-wolfram-language},
	abstract = {A plugin for Wolfram Language development. This plugin turns IntelliJ platform based products into powerful coding environments for the Wolfram Language. It provides...},
	urldate = {2024-02-24},
	journal = {JetBrains Marketplace},
}

@misc{noauthor_switchwolfram_nodate,
	title = {Switch—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Switch.html},
	urldate = {2024-02-21},
}

@article{greff_lstm_2017,
	title = {{LSTM}: {A} {Search} {Space} {Odyssey}},
	volume = {28},
	issn = {2162-2388},
	shorttitle = {{LSTM}},
	url = {https://ieeexplore.ieee.org/document/7508408},
	doi = {10.1109/TNNLS.2016.2582924},
	abstract = {Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ( {\textbackslash}approx 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
	number = {10},
	urldate = {2024-02-17},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Greff, Klaus and Srivastava, Rupesh K. and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
	month = oct,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Computer architecture, Functional ANalysis Of VAriance (fANOVA), Handwriting recognition, Logic gates, Microprocessors, Recurrent neural networks, Speech recognition, Training, long short-term memory (LSTM), random search, recurrent neural networks, sequence learning},
	pages = {2222--2232},
}

@inproceedings{papineni_bleu_2002-1,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2024-01-29},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	editor = {Isabelle, Pierre and Charniak, Eugene and Lin, Dekang},
	month = jul,
	year = {2002},
	pages = {311--318},
}

@misc{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	doi = {10.48550/arXiv.1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2024-01-29},
	publisher = {arXiv},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = dec,
	year = {2014},
	note = {arXiv:1409.3215 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	doi = {10.48550/arXiv.1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2024-01-29},
	publisher = {arXiv},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv:1406.1078 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{gu_mamba_nodate,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers’ computational ineﬃciency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of eﬃcient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpliﬁed end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	language = {en},
	author = {Gu, Albert and Dao, Tri},
}

@misc{liu_tuning_2024,
	title = {Tuning {Language} {Models} by {Proxy}},
	url = {http://arxiv.org/abs/2401.08565},
	abstract = {Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the result of directly tuning the model, but by accessing only its prediction over the output vocabulary. Our method instead tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the base model in the direction of tuning, while retaining the benefits of larger scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88\% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. Interestingly, when tested on TruthfulQA, proxy-tuned models are actually more truthful than directly tuned models, possibly because decoding-time guidance better retains the model's factual knowledge. We then demonstrate the generality of proxy-tuning by applying it for domain adaptation on code, and task-specific finetuning on question-answering and math problems. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Liu, Alisa and Han, Xiaochuang and Wang, Yizhong and Tsvetkov, Yulia and Choi, Yejin and Smith, Noah A.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.08565 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{gu_mamba_2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	issn = {1533-7928},
	shorttitle = {Dropout},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {56},
	urldate = {2024-01-12},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
}

@misc{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {https://arxiv.org/abs/1412.6980v9},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	language = {en},
	urldate = {2024-01-12},
	journal = {arXiv.org},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
}

@misc{britz_massive_2017,
	title = {Massive {Exploration} of {Neural} {Machine} {Translation} {Architectures}},
	url = {https://arxiv.org/abs/1703.03906v2},
	abstract = {Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.},
	language = {en},
	urldate = {2024-01-12},
	journal = {arXiv.org},
	author = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
	month = mar,
	year = {2017},
}

@misc{yang_if_2024,
	title = {If {LLM} {Is} the {Wizard}, {Then} {Code} {Is} the {Wand}: {A} {Survey} on {How} {Code} {Empowers} {Large} {Language} {Models} to {Serve} as {Intelligent} {Agents}},
	shorttitle = {If {LLM} {Is} the {Wizard}, {Then} {Code} {Is} the {Wand}},
	url = {http://arxiv.org/abs/2401.00812},
	abstract = {The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.},
	urldate = {2024-01-11},
	publisher = {arXiv},
	author = {Yang, Ke and Liu, Jiateng and Wu, John and Yang, Chaoqi and Fung, Yi R. and Li, Sha and Huang, Zixuan and Cao, Xu and Wang, Xingyao and Wang, Yiquan and Ji, Heng and Zhai, Chengxiang},
	month = jan,
	year = {2024},
	note = {arXiv:2401.00812 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{luong_effective_2015-1,
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1508.04025},
	doi = {10.48550/arXiv.1508.04025},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	urldate = {2024-01-11},
	publisher = {arXiv},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	month = sep,
	year = {2015},
	note = {arXiv:1508.04025 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{luong_effective_2015-2,
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1508.04025},
	doi = {10.48550/arXiv.1508.04025},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	urldate = {2024-01-11},
	publisher = {arXiv},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	month = sep,
	year = {2015},
	note = {arXiv:1508.04025 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2024-01-11},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{nasr_scalable_2023,
	title = {Scalable {Extraction} of {Training} {Data} from ({Production}) {Language} {Models}},
	url = {http://arxiv.org/abs/2311.17035},
	abstract = {This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and Choquette-Choo, Christopher A. and Wallace, Eric and Tramèr, Florian and Lee, Katherine},
	month = nov,
	year = {2023},
	note = {arXiv:2311.17035 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{metz_microsoft_2023,
	chapter = {Technology},
	title = {Microsoft {Bets} {Big} on the {Creator} of {ChatGPT} in {Race} to {Dominate} {A}.{I}.},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2023/01/12/technology/microsoft-openai-chatgpt.html},
	abstract = {As a new chatbot wows the world with its conversational talents, a resurgent tech giant is poised to reap the benefits while doubling down on a relationship with the start-up OpenAI.},
	language = {en-US},
	urldate = {2023-12-11},
	journal = {The New York Times},
	author = {Metz, Cade and Weise, Karen},
	month = jan,
	year = {2023},
	keywords = {Altman, Samuel H, Artificial Intelligence, ChatGPT, Cloud Computing, Computers and the Internet, Microsoft Corp, Nadella, Satya, OpenAI Labs, Research, future},
}

@article{griffith_let_2023,
	chapter = {Technology},
	title = {‘{Let} 1,000 {Flowers} {Bloom}’: {A}.{I}. {Funding} {Frenzy} {Escalates}},
	issn = {0362-4331},
	shorttitle = {‘{Let} 1,000 {Flowers} {Bloom}’},
	url = {https://www.nytimes.com/2023/03/14/technology/ai-funding-boom.html},
	abstract = {In just weeks, a gold rush into artificial intelligence start-ups has become a full-blown mania.},
	language = {en-US},
	urldate = {2023-12-11},
	journal = {The New York Times},
	author = {Griffith, Erin and Metz, Cade},
	month = mar,
	year = {2023},
	keywords = {Anthropic AI LLC, Artificial Intelligence, ChatGPT, Cohere Inc, Computers and the Internet, Entrepreneurship, OpenAI Labs, Start-ups, Venture Capital},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{jozefowicz_exploring_2016,
	title = {Exploring the {Limits} of {Language} {Modeling}},
	url = {http://arxiv.org/abs/1602.02410},
	doi = {10.48550/arXiv.1602.02410},
	abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
	urldate = {2023-12-10},
	publisher = {arXiv},
	author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
	month = feb,
	year = {2016},
	note = {arXiv:1602.02410 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{paperno_lambada_2016,
	title = {The {LAMBADA} dataset: {Word} prediction requiring a broad discourse context},
	shorttitle = {The {LAMBADA} dataset},
	url = {http://arxiv.org/abs/1606.06031},
	doi = {10.48550/arXiv.1606.06031},
	abstract = {We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1\% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.},
	urldate = {2023-12-10},
	publisher = {arXiv},
	author = {Paperno, Denis and Kruszewski, Germán and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fernández, Raquel},
	month = jun,
	year = {2016},
	note = {arXiv:1606.06031 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{marcus_penn_1994,
	title = {The {Penn} {Treebank}: {Annotating} {Predicate} {Argument} {Structure}},
	shorttitle = {The {Penn} {Treebank}},
	url = {https://aclanthology.org/H94-1020},
	urldate = {2023-12-10},
	booktitle = {Human {Language} {Technology}: {Proceedings} of a {Workshop} held at {Plainsboro}, {New} {Jersey}, {March} 8-11, 1994},
	author = {Marcus, Mitchell and Kim, Grace and Marcinkiewicz, Mary Ann and MacIntyre, Robert and Bies, Ann and Ferguson, Mark and Katz, Karen and Schasberger, Britta},
	year = {1994},
}

@misc{liu_generating_2018,
	title = {Generating {Wikipedia} by {Summarizing} {Long} {Sequences}},
	url = {http://arxiv.org/abs/1801.10198},
	doi = {10.48550/arXiv.1801.10198},
	abstract = {We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
	urldate = {2023-12-10},
	publisher = {arXiv},
	author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
	month = jan,
	year = {2018},
	note = {arXiv:1801.10198 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{child_generating_2019,
	title = {Generating {Long} {Sequences} with {Sparse} {Transformers}},
	url = {http://arxiv.org/abs/1904.10509},
	doi = {10.48550/arXiv.1904.10509},
	abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n {\textbackslash}sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
	month = apr,
	year = {2019},
	note = {arXiv:1904.10509 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{sharir_cost_2020,
	title = {The {Cost} of {Training} {NLP} {Models}: {A} {Concise} {Overview}},
	shorttitle = {The {Cost} of {Training} {NLP} {Models}},
	url = {http://arxiv.org/abs/2004.08900},
	doi = {10.48550/arXiv.2004.08900},
	abstract = {We review the cost of training large-scale language models, and the drivers of these costs. The intended audience includes engineers and scientists budgeting their model-training experiments, as well as non-practitioners trying to make sense of the economics of modern-day Natural Language Processing (NLP).},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Sharir, Or and Peleg, Barak and Shoham, Yoav},
	month = apr,
	year = {2020},
	note = {arXiv:2004.08900 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
}

@misc{sennrich_neural_2016,
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://arxiv.org/abs/1508.07909},
	doi = {10.48550/arXiv.1508.07909},
	abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = jun,
	year = {2016},
	note = {arXiv:1508.07909 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{mccann_learned_2017,
	title = {Learned in {Translation}: {Contextualized} {Word} {Vectors}},
	volume = {30},
	shorttitle = {Learned in {Translation}},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/20c86a628232a67e7bd46f76fba7ce12-Abstract.html},
	abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
	urldate = {2023-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
	year = {2017},
}

@article{bengio_neural_nodate,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difﬁcult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to ﬁght the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a signiﬁcant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach signiﬁcantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
	language = {en},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
}

@article{bengio_neural_nodate-1,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difﬁcult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to ﬁght the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a signiﬁcant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach signiﬁcantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
	language = {en},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@misc{mccann_natural_2018,
	title = {The {Natural} {Language} {Decathlon}: {Multitask} {Learning} as {Question} {Answering}},
	shorttitle = {The {Natural} {Language} {Decathlon}},
	url = {http://arxiv.org/abs/1806.08730},
	abstract = {Deep learning has improved performance on many natural language processing (NLP) tasks individually. However, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task. We introduce the Natural Language Decathlon (decaNLP), a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. We cast all tasks as question answering over a context. Furthermore, we present a new Multitask Question Answering Network (MQAN) jointly learns all tasks in decaNLP without any task-specific modules or parameters in the multitask setting. MQAN shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification. We demonstrate that the MQAN's multi-pointer-generator decoder is key to this success and performance further improves with an anti-curriculum training strategy. Though designed for decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic parsing task in the single-task setting. We also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for decaNLP.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {McCann, Bryan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
	month = jun,
	year = {2018},
	note = {arXiv:1806.08730 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hendrycks_pretrained_2020,
	title = {Pretrained {Transformers} {Improve} {Out}-of-{Distribution} {Robustness}},
	url = {http://arxiv.org/abs/2004.06100},
	doi = {10.48550/arXiv.2004.06100},
	abstract = {Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Liu, Xiaoyuan and Wallace, Eric and Dziedzic, Adam and Krishnan, Rishabh and Song, Dawn},
	month = apr,
	year = {2020},
	note = {arXiv:2004.06100 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2023-12-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}

@misc{brown_language_2020-1,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
}

@misc{sebastian_raschka_l19525_2021,
	title = {L19.5.2.5 {GPT}-v3: {Language} {Models} are {Few}-{Shot} {Learners}},
	shorttitle = {L19.5.2.5 {GPT}-v3},
	url = {https://www.youtube.com/watch?v=wYdKn-X4MhY},
	abstract = {Slides: https://sebastianraschka.com/pdf/lect...

-------

This video is part of my Introduction of Deep Learning course.

Next video:    • L19.5.2.6 BART:  Combining Bidirectio...  

The complete playlist:    • Intro to Deep Learning and Generative...  

A handy overview page with links to the materials: https://sebastianraschka.com/blog/202...

-------

If you want to be notified about future videos, please consider subscribing to my channel:    / sebastianraschka},
	urldate = {2023-12-06},
	author = {{Sebastian Raschka}},
	month = may,
	year = {2021},
}

@misc{aleksa_gordic_-_the_ai_epiphany_gpt-3_2020,
	title = {{GPT}-3 - {Language} {Models} are {Few}-{Shot} {Learners} {\textbar} {Paper} {Explained}},
	url = {https://www.youtube.com/watch?v=fVt387VZJe8},
	abstract = {❤️ Become The AI Epiphany Patreon ❤️ ►   / theaiepiphany  
▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬

In this video, I cover the famous GPT-3 model.

I first give you some context about the stuff that happened since the paper was first published in May 2020 (hype, anti-hype, limitations, and cool apps), and then I dive deep into explaining the paper.

You'll learn about:
✔️ Useful resources on GPT-3
✔️ Main takeaways from the paper

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ 
✅ "anti-hype" blog: https://lacker.io/ai/2020/07/06/givin...
✅ Gwern's blog: https://www.gwern.net/GPT-3
✅ My transformer implementation: https://github.com/gordicaleksa/pytor...
✅ Cool "GPT game": https://play.aidungeon.io/
▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ 

⌚️ Timetable:

00:00 GPT (anti)hype, Gwern, prompt programming
04:30 Abstract of the paper
06:50 Architecture, data, compute
12:15 Zero-shot, one-shot, and few-shot learning
18:45 Power-law chart (more compute please)
20:35 Results (machine translation)
23:05 NLI (reasoning is hard)
24:40 Arithmetic
26:25 Word unscrambling
28:40 SAT analogies (how smart are humans?)
30:45 Fake news generation
32:05 Data contamination
35:05 Limitations of the model
37:35 Bias, fairness (broader impact)
44:30 Final thoughts, are we going towards an AGI?

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ 
💰 BECOME A PATREON OF THE AI EPIPHANY ❤️ 

If these videos, GitHub projects, and blogs help you,
consider helping me out by supporting me on Patreon! 

The AI Epiphany ►   / theaiepiphany  
One-time donation: 
https://www.paypal.com/paypalme/theai... 

Much love! ❤️

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ 

💡 The AI Epiphany is a channel dedicated to simplifying the field of AI using creative visualizations and in general, a stronger focus on geometrical and visual intuition, rather than the algebraic and numerical "intuition". 

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬ 
👋 CONNECT WITH ME ON SOCIAL 
LinkedIn ►   / aleksagordic   
Twitter ►   / gordic\_aleksa   

Instagram ►   / aiepiphany   
Facebook ►   / aiepiphany   

👨‍👩‍👧‍👦 JOIN OUR DISCORD COMMUNITY:
Discord ►   / discord  

📢 SUBSCRIBE TO MY MONTHLY AI NEWSLETTER:
Substack ► https://aiepiphany.substack.com/

💻 FOLLOW ME ON GITHUB FOR COOL PROJECTS: 
GitHub ► https://github.com/gordicaleksa 

📚 FOLLOW ME ON MEDIUM:
Medium ►   / gordicaleksa  
▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬

\#gpt3 \#fewshotlearning \#deeplearning},
	urldate = {2023-12-06},
	author = {{Aleksa Gordić - The AI Epiphany}},
	month = nov,
	year = {2020},
}

@misc{yannic_kilcher_gpt-3_2020,
	title = {{GPT}-3: {Language} {Models} are {Few}-{Shot} {Learners} ({Paper} {Explained})},
	shorttitle = {{GPT}-3},
	url = {https://www.youtube.com/watch?v=SY5PvZrJhLE},
	abstract = {\#gpt3 \#openai \#gpt-3

How far can you go with ONLY language modeling? Can a large enough language model perform NLP task out of the box? OpenAI take on these and other questions by training a transformer that is an order of magnitude larger than anything that has ever been built before and the results are astounding.

OUTLINE:
0:00 - Intro \& Overview
1:20 - Language Models
2:45 - Language Modeling Datasets
3:20 - Model Size
5:35 - Transformer Models
7:25 - Fine Tuning
10:15 - In-Context Learning
17:15 - Start of Experimental Results
19:10 - Question Answering
23:10 - What I think is happening
28:50 - Translation
31:30 - Winograd Schemes
33:00 - Commonsense Reasoning
37:00 - Reading Comprehension
37:30 - SuperGLUE
40:40 - NLI
41:40 - Arithmetic Expressions
48:30 - Word Unscrambling
50:30 - SAT Analogies
52:10 - News Article Generation
58:10 - Made-up Words
1:01:10 - Training Set Contamination
1:03:10 - Task Examples

https://arxiv.org/abs/2005.14165
https://github.com/openai/gpt-3

Abstract:
Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.

Authors: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei

Links:
YouTube:    / yannickilcher  
Twitter:   / ykilcher  
BitChute: https://www.bitchute.com/channel/yann...
Minds: https://www.minds.com/ykilcher},
	urldate = {2023-12-06},
	author = {{Yannic Kilcher}},
	month = may,
	year = {2020},
}

@misc{bottou_borges_2023,
	title = {Borges and {AI}},
	url = {http://arxiv.org/abs/2310.01425},
	doi = {10.48550/arXiv.2310.01425},
	abstract = {Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Bottou, Léon and Schölkopf, Bernhard},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01425 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2023-11-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
}

@misc{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = dec,
	year = {2014},
	note = {arXiv:1409.3215 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wang_voyager_2023,
	title = {Voyager: {An} {Open}-{Ended} {Embodied} {Agent} with {Large} {Language} {Models}},
	shorttitle = {Voyager},
	url = {http://arxiv.org/abs/2305.16291},
	abstract = {We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	month = may,
	year = {2023},
	note = {arXiv:2305.16291 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{wang_voyager_2023-1,
	title = {Voyager: {An} {Open}-{Ended} {Embodied} {Agent} with {Large} {Language} {Models}},
	shorttitle = {Voyager},
	url = {http://arxiv.org/abs/2305.16291},
	doi = {10.48550/arXiv.2305.16291},
	abstract = {We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	month = may,
	year = {2023},
	note = {arXiv:2305.16291 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{noauthor_can_nodate,
	title = {Can {Global} {Semantic} {Context} {Improve} {Neural} {Language} {Models}?},
	url = {https://machinelearning.apple.com/research/can-global-semantic-context-improve-neural-language-models},
	abstract = {Entering text on your iPhone, discovering news articles you might enjoy, finding out answers to questions you may have, and many other…},
	language = {en-US},
	urldate = {2023-10-16},
	journal = {Apple Machine Learning Research},
}

@article{windsteiger_theorema_2013,
	title = {Theorema 2.0: {A} {Graphical} {User} {Interface} for a {Mathematical} {Assistant} {System}},
	volume = {118},
	issn = {2075-2180},
	shorttitle = {Theorema 2.0},
	url = {http://arxiv.org/abs/1307.1945},
	doi = {10.4204/EPTCS.118.5},
	abstract = {Theorema 2.0 stands for a re-design including a complete re-implementation of the Theorema system, which was originally designed, developed, and implemented by Bruno Buchberger and his Theorema group at RISC. In this paper, we present the first prototype of a graphical user interface (GUI) for the new system. It heavily relies on powerful interactive capabilities introduced in recent releases of the underlying Mathematica system, most importantly the possibility of having dynamic objects connected to interface elements like sliders, menus, check-boxes, radio-buttons and the like. All these features are fully integrated into the Mathematica programming environment and allow the implementation of a modern user interface.},
	urldate = {2023-10-14},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Windsteiger, Wolfgang},
	month = jul,
	year = {2013},
	note = {arXiv:1307.1945 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Mathematical Software, Computer Science - Symbolic Computation},
	pages = {72--82},
}

@misc{wolfram_research_patternswolfram_nodate,
	title = {Patterns—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/Patterns.html},
	urldate = {2023-10-14},
	author = {{Wolfram Research}},
}

@misc{wolfram_research_patterns_nodate,
	title = {Patterns and {Transformation} {Rules}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/PatternsAndTransformationRules.html},
	urldate = {2023-10-14},
	author = {{Wolfram Research}},
}

@misc{wolfram_research_notebookfilenamewolfram_nodate,
	title = {{NotebookFileName}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/NotebookFileName.html},
	urldate = {2023-10-14},
	author = {{Wolfram Research}},
}

@misc{wolfram_research_manipulating_nodate,
	title = {Manipulating {Notebooks}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/tutorial/ManipulatingNotebooks.html},
	urldate = {2023-10-14},
	author = {{Wolfram Research}},
}

@misc{wolfram_research_generate_nodate,
	title = {Generate {TeX} with the {Wolfram} {Language}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/workflow/GenerateTeXWithTheWolframLanguage.html},
	urldate = {2023-10-14},
	author = {{Wolfram Research}},
}

@misc{wolfram_research_export_nodate,
	title = {Export: {Output} data to a specified file format—{Wolfram} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Export.html},
	urldate = {2023-10-14},
	author = {{Wolfram Research}},
}

@misc{wolfram_research_module_nodate,
	title = {Module: {Create} a scoping construct for local variables—{Wolfram} {Documentation}},
	url = {https://reference.wolfram.com/language/ref/Module.html},
	urldate = {2023-10-14},
	author = {{Wolfram Research}},
}

@misc{wolfram_research_modularity_nodate,
	title = {Modularity and the {Naming} of {Things}},
	shorttitle = {Modularity},
	url = {https://reference.wolfram.com/language/tutorial/ModularityAndTheNamingOfThings.html},
	urldate = {2023-10-14},
	author = {{Wolfram Research}},
}

@incollection{dershowitz_chapter_1990,
	address = {Amsterdam},
	series = {Handbook of {Theoretical} {Computer} {Science}},
	title = {{CHAPTER} 6 - {Rewrite} {Systems}},
	isbn = {978-0-444-88074-1},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444880741500111},
	abstract = {This chapter focuses on rewrite systems, which are directed equations used to compute by repeatedly replacing sub-terms of a given formula with equal terms until the simplest form possible is obtained. As a formalism, rewrite systems have the full power of Turing machines and may be thought of as nondeterministic Markov algorithms over terms rather than strings. The theory of rewriting is in essence a theory of normal forms. To some extent, it is an outgrowth of the study of A. Church's Lambda Calculus and H. B. Curry's Combinatory Logic. The chapter discusses the syntax and semantics of equations from the algebraic, logical, and operational points of view. To use a rewrite system as a decision procedure, it must be convergent. The chapter describes this fundamental concept as an abstract property of binary relations. To use a rewrite system for computation or as a decision procedure for validity of identities, the termination property is crucial. The chapter presents the basic methods for proving termination. The chapter discusses the question of satisfiability of equations and the convergence property applied to rewriting.},
	urldate = {2023-10-14},
	booktitle = {Formal {Models} and {Semantics}},
	publisher = {Elsevier},
	author = {Dershowitz, Nachum and Jouannaud, Jean-Pierre},
	editor = {Van leeuwen, JAN},
	month = jan,
	year = {1990},
	doi = {10.1016/B978-0-444-88074-1.50011-1},
	pages = {243--320},
}

@article{marin_rule-based_2004,
	title = {Rule-{Based} {Programming} with {Mathematica} {Mircea} {Marin}},
	abstract = {Recent years have witnessed renewed developments of the rule-based programming style, addressing both its theoretical foundations and its practical implementations. New rule-based programming languages have emerged, and several practical applications have shown that rules are indeed a useful programming tool. We believe that Mathematica has the right basic ingredients for supporting rule-based programming efficiently. Because the main features of a true rule-based programming language are still missing, we developed a Mathematica package, \$ ho\$Log, which enables advanced rule-based programming within Mathematica. We describe here the capabilities of \$ ho\$Log and illustrate its usage with several examples.},
	author = {Marin, Mircea and Piroi, Florina},
	month = may,
	year = {2004},
}

@techreport{wolfram_research_highlevel_nodate,
	type = {What's {New}},
	title = {High–{Level} {Functions}: {New} in {Wolfram} {Language} 12},
	shorttitle = {High–{Level} {Functions}},
	url = {https://www.wolfram.com/language/12/built-in-interface-to-unity-game-engine/high-level-functions.html?product=language},
	language = {en},
	urldate = {2023-10-14},
	author = {{Wolfram Research}},
}

@article{buchberger_mathematica_1996,
	title = {Mathematica as a {Rewrite} {Language}},
	author = {Buchberger, Bruno},
	month = nov,
	year = {1996},
}

@misc{bruno_buchberger_1996-mathematicaasarewritelanguagepdf_nodate,
	title = {1996-{MathematicaasaRewriteLanguage}.pdf},
	author = {{Bruno Buchberger}},
}

@misc{jack_heseltine_zotero_nodate,
	title = {Zotero {Folder} {Test}},
	author = {{Jack Heseltine}},
}
